# üß© M√≥dulo 4: Estructuras de Datos y Algoritmos B√°sicos
## **Secci√≥n 9: Algoritmos de Aprendizaje Autom√°tico**

---

### üß† Introducci√≥n General

El **aprendizaje autom√°tico (Machine Learning)** es una rama de la inteligencia artificial que permite a los sistemas **aprender patrones a partir de datos**, sin ser expl√≠citamente programados para cada tarea.
Los algoritmos de ML se clasifican en:

- **Supervisado:** aprende a partir de ejemplos con etiqueta (p. ej. regresi√≥n, clasificaci√≥n).
- **No supervisado:** encuentra estructura o agrupamientos sin etiquetas (p. ej. K-Means).
- **Por refuerzo:** aprende mediante recompensas o castigos (p. ej. aprendizaje de pol√≠ticas).

---

## üîπ 9.1. K-Means (Agrupamiento No Supervisado)

**Objetivo / Aplicaci√≥n:**
Agrupar un conjunto de datos en **K grupos** donde cada punto pertenece al centroide m√°s cercano.

**Fundamento te√≥rico:**
Minimiza la suma de distancias cuadr√°ticas entre los puntos y su centroide.
**Complejidad:** O(n¬∑k¬∑i) (n puntos, k grupos, i iteraciones) ¬∑ **Clasificaci√≥n:** No supervisado / Iterativo
**Origen:** **MacQueen (1967)**.

```mermaid
flowchart TD
A[Inicializar K centroides] --> B[Asignar puntos al centroide m√°s cercano]
B --> C[Recalcular centroides]
C --> D{Centroides estables?}
D -->|No| B
D -->|S√≠| E[Fin del agrupamiento]
```

```python
import random

def kmeans(data, k=2, iters=5):
    centroids = random.sample(data, k)
    for _ in range(iters):
        grupos = {i: [] for i in range(k)}
        for x in data:
            dists = [abs(x-c) for c in centroids]
            grupo = dists.index(min(dists))
            grupos[grupo].append(x)
        centroids = [sum(v)/len(v) for v in grupos.values() if v]
    return centroids, grupos

datos = [1,2,3,10,11,12]
print(kmeans(datos, k=2))
```

---

## üîπ 9.2. Naive Bayes

**Objetivo / Aplicaci√≥n:**
Clasificar elementos bas√°ndose en la **probabilidad condicional**. Muy usado en **filtrado de spam, an√°lisis de texto y detecci√≥n de sentimientos**.

**Fundamento te√≥rico:**
Usa el **Teorema de Bayes** bajo el supuesto de independencia entre variables:
\( P(C|X) = \frac{P(X|C)¬∑P(C)}{P(X)} \)
**Complejidad:** O(n¬∑m) (n muestras, m atributos) ¬∑ **Clasificaci√≥n:** Supervisado / Probabil√≠stico
**Origen:** Basado en el trabajo de **Thomas Bayes (1763)**.

```mermaid
flowchart TD
A[Entrenamiento] --> B[Calcular P(C) y P(X|C)]
B --> C[Predicci√≥n]
C --> D[Aplicar Teorema de Bayes]
D --> E[Seleccionar clase con mayor probabilidad]
```

```python
def naive_bayes(text, clases):
    palabras = set(sum(clases.values(), []))
    P = {}
    for c, lista in clases.items():
        P[c] = sum(text.count(w) for w in lista) / len(palabras)
    return max(P, key=P.get)

clases = {
    "positivo": ["bueno","excelente","genial"],
    "negativo": ["malo","horrible","terrible"]
}
print(naive_bayes("excelente servicio", clases))
```

---

## üîπ 9.3. Support Vector Machines (SVM)

**Objetivo / Aplicaci√≥n:**
Encontrar un **hiperplano √≥ptimo** que separe los datos de diferentes clases con el **m√°ximo margen** posible.

**Fundamento te√≥rico:**
Busca maximizar \( \frac{2}{||w||} \) sujeto a que los puntos est√©n correctamente clasificados:
\( y_i(w¬∑x_i + b) ‚â• 1 \).
**Complejidad:** O(n¬≥) ¬∑ **Clasificaci√≥n:** Supervisado / Geom√©trico / Determin√≠stico
**Origen:** **Vapnik y Chervonenkis (1992)**.

```mermaid
flowchart TD
A[Datos etiquetados] --> B[Calcular margen √≥ptimo]
B --> C[Encontrar soporte vectorial]
C --> D[Construir hiperplano separador]
D --> E[Clasificar nuevos puntos]
```

```python
# Ejemplo 1D simple
def svm_predict(x, w, b):
    return 1 if w*x + b >= 0 else -1

w, b = 0.5, -2
puntos = [1,3,6]
for x in puntos:
    print(x, "-> clase", svm_predict(x, w, b))
```

---

## üîπ 9.4. √Årboles de Decisi√≥n y Bosques Aleatorios

**Objetivo / Aplicaci√≥n:**
Construir reglas jer√°rquicas de decisi√≥n para clasificar datos o realizar regresi√≥n.

**Fundamento te√≥rico:**
Divide el espacio de atributos en regiones homog√©neas minimizando la **entrop√≠a** o el **√≠ndice Gini**.
**Complejidad:** O(n¬∑m¬∑log n) ¬∑ **Clasificaci√≥n:** Supervisado / No param√©trico
**Origen:** **Breiman et al. (1984)**.

```mermaid
flowchart TD
A[Conjunto de datos] --> B[Seleccionar atributo √≥ptimo]
B --> C[Dividir nodo seg√∫n valor]
C --> D{Nodo puro?}
D -->|No| B
D -->|S√≠| E[Asignar clase]
```

```python
# √Årbol simple con 1 atributo: si la temperatura > 25 => 'calor'
def arbol_decision(temp):
    return "calor" if temp > 25 else "frio"

print(arbol_decision(30))
print(arbol_decision(18))
```

---

## üîπ 9.5. Backpropagation (Retropropagaci√≥n del Error)

**Objetivo / Aplicaci√≥n:**
Entrenar **redes neuronales artificiales** ajustando los pesos para minimizar el error entre la salida predicha y la deseada.

### üåê Fundamento Te√≥rico

Una red neuronal est√° formada por capas de **neuronas interconectadas**.
Cada neurona aplica una funci√≥n de activaci√≥n (sigmoide, ReLU, etc.) sobre una suma ponderada de entradas.

Durante el entrenamiento:
1. Se propagan los datos hacia adelante (**forward pass**).
2. Se calcula el error respecto a la salida real.
3. Se retropropaga el error (**backward pass**) actualizando los pesos mediante el **gradiente descendente**.

**Ecuaci√≥n de actualizaci√≥n:**
$$w_{ij} := w_{ij} - \alpha ¬∑ \frac{‚àÇE}{‚àÇw_{ij}} $$
donde $$ E = \frac{1}{2}(y_{real} - y_{pred})^2 $$

**Complejidad:** O(n¬∑m¬∑e) (n neuronas, m conexiones, e √©pocas)
**Clasificaci√≥n:** Supervisado / Iterativo / Num√©rico
**Origen:** **Rumelhart, Hinton y Williams (1986)**
**Importancia actual:** Es la base del **Deep Learning moderno** (redes convolucionales, transformers, etc.).

```mermaid
flowchart TD
A[Entrada x] --> B[Forward Pass: calcular salida]
B --> C[Comparar con y_real]
C --> D[Calcular error y gradientes]
D --> E[Actualizar pesos con gradiente descendente]
E --> F[Repetir por √©pocas]
```

```python
import math

def sigmoid(x): return 1/(1+math.exp(-x))
def d_sigmoid(y): return y*(1-y)

# Red con 1 neurona: aprende la funci√≥n identidad
w = 0.5
alpha = 0.1
datos = [(0,0),(1,1)]
for epoch in range(1000):
    total_error = 0
    for x,y in datos:
        # forward
        y_pred = sigmoid(x*w)
        # error
        error = y - y_pred
        # backward
        w += alpha * error * d_sigmoid(y_pred) * x
        total_error += error**2
    if epoch % 200 == 0:
        print(f"√©poca {epoch} error {total_error:.4f} w={w:.3f}")

print("Predicci√≥n(1):", sigmoid(1*w))
```

### üîç Interpretaci√≥n

El algoritmo ajusta progresivamente los pesos de la red hasta que las predicciones se aproximan al valor real.
La **retropropagaci√≥n** es esencial para el entrenamiento de **redes profundas**, donde m√∫ltiples capas de neuronas se optimizan en conjunto.

---

### ‚úÖ Cierre de la Secci√≥n 9

El aprendizaje autom√°tico representa un paradigma donde los algoritmos **extraen conocimiento a partir de datos**.
- **K-Means:** agrupa sin supervisi√≥n.
- **Naive Bayes / SVM / √Årboles:** clasifican de forma supervisada.
- **Backpropagation:** coraz√≥n del **Deep Learning** moderno.

---
