# üß™ M√≥dulo 8: Introducci√≥n pr√°ctica a Ciencia de Datos con Python
## **Tema 8.1:** NumPy y Pandas (series, dataframes, joins, groupby)

> **Objetivo del tema:** Comprender el rol de Python en ciencia de datos y dominar los fundamentos pr√°cticos de **NumPy** y **Pandas** para manipulaci√≥n, limpieza, an√°lisis y agregaci√≥n de datos. Al finalizar, podr√°s cargar datos, transformarlos eficientemente y responder preguntas con res√∫menes, joins y groupby.

---

## 1) ¬øQu√© es Ciencia de Datos y por qu√© importa?
La **ciencia de datos** combina estad√≠stica, programaci√≥n y conocimiento del dominio para **extraer valor** de los datos. Permite:
- **Tomar decisiones informadas** (presupuestos, inventarios, calidad, marketing).
- **Detectar patrones y anomal√≠as** (fallas, fraudes, cuellos de botella).
- **Optimizar procesos** (costos, tiempos, rutas, mantenimiento).
- **Construir modelos predictivos** (demanda, churn, pron√≥stico de series).

**Hoy** es clave por la abundancia de datos (sensores, ERPs, CRMs, web) y el bajo costo de c√≥mputo.

---

## 2) ¬øPor qu√© Python?
- **Ecosistema maduro:** NumPy, Pandas, SciPy, scikit-learn, Matplotlib/Plotly, Jupyter.
- **Curva de aprendizaje** accesible y comunidad enorme.
- **Integraci√≥n** con bases de datos, APIs, cloud, big data (PySpark), y producci√≥n (FastAPI).
- **Rapidez de desarrollo**: prototipos √°giles + buen rendimiento v√≠a **vectorizaci√≥n** (NumPy) y C bajo el cap√≥.

**Herramientas del ecosistema:**
- **C√≥mputo num√©rico:** NumPy, SciPy.
- **Manipulaci√≥n tabular:** Pandas, (alternativas: Polars, Dask).
- **ML/IA:** scikit-learn, XGBoost, LightGBM, PyTorch, TensorFlow.
- **Visualizaci√≥n:** Matplotlib, Plotly, Altair.
- **Entornos:** Jupyter, VS Code, marimo, Spyder.
- **Datos grandes:** Dask, PySpark.
- **APIs & producci√≥n:** FastAPI, Pydantic, Docker.

---

## 3) NumPy: fundamentos esenciales

### 3.0 ¬øQu√© es NumPy y qu√© problema resuelve?

**NumPy** (*Numerical Python*) es la base del c√≥mputo cient√≠fico en Python. Provee el tipo **ndarray** (arreglo n-dimensional) y funciones para operaciones matem√°ticas de alto rendimiento.

#### Filosof√≠a y prop√≥sito
- **Rendimiento:** NumPy est√° escrito en C, permitiendo operaciones vectorizadas hasta **100x m√°s r√°pidas** que bucles Python puros.
- **Memoria eficiente:** Los arrays almacenan datos contiguos en memoria con tipos homog√©neos (int32, float64, etc.), reduciendo overhead.
- **Expresividad matem√°tica:** Sintaxis concisa para operaciones matriciales, broadcasting y √°lgebra lineal.
- **Base del ecosistema:** Pandas, SciPy, scikit-learn, TensorFlow y PyTorch usan NumPy internamente.

#### ¬øQu√© problema resuelve?
Python puro es **lento** para operaciones num√©ricas masivas porque:
- Las listas son **heterog√©neas** y cada elemento tiene metadata (tipo, contador de referencias).
- Los bucles `for` interpretan cada iteraci√≥n individualmente.

**Ejemplo del problema:**
```python
# Python puro: sumar dos listas de 1 mill√≥n de elementos
a = list(range(1_000_000))
b = list(range(1_000_000))
c = [a[i] + b[i] for i in range(1_000_000)]  # ~100ms

# NumPy: vectorizaci√≥n pura
import numpy as np
a = np.arange(1_000_000)
b = np.arange(1_000_000)
c = a + b  # ~1ms (100x m√°s r√°pido)
```

**NumPy resuelve:**
1. **Lentitud** con vectorizaci√≥n (operaciones sobre arrays completos).
2. **Uso excesivo de memoria** con tipos homog√©neos y contiguidad.
3. **Complejidad de c√≥digo** con broadcasting (operaciones sobre distintas formas).
4. **Falta de funciones matem√°ticas** (√°lgebra lineal, FFT, estad√≠sticas).

---

### 3.1 Creaci√≥n y propiedades
```python
import numpy as np

# Creaci√≥n desde listas
a = np.array([1, 2, 3], dtype=np.int64)
print(a.ndim, a.shape, a.dtype)  # 1, (3,), int64

# Matrices inicializadas
b = np.zeros((2, 3))          # matriz 2x3 de ceros (float64 por defecto)
c = np.ones(5, dtype=np.float32)
d = np.full((3, 3), 7)        # matriz 3x3 llena de 7s

# Rangos y secuencias
e = np.arange(0, 10, 2)       # [0, 2, 4, 6, 8] (como range)
f = np.linspace(0, 1, 5)      # [0.0, 0.25, 0.5, 0.75, 1.0] (5 puntos equiespaciados)

# Matrices especiales
identidad = np.eye(3)         # matriz identidad 3x3
diagonal = np.diag([1, 2, 3]) # matriz diagonal

# Ejemplo pr√°ctico: simular temperaturas de una semana
temperaturas = np.random.randint(15, 30, size=7)  # 7 d√≠as, entre 15¬∞C y 30¬∞C
print(f"Temperaturas: {temperaturas}")
print(f"Promedio: {temperaturas.mean():.1f}¬∞C")
```

### 3.2 Indexado, slicing y m√°scaras
```python
# Crear matriz de ejemplo: tabla de ventas (filas=productos, columnas=meses)
ventas = np.array([
    [100, 120, 150, 130],  # Producto A
    [80,  90,  95,  100],  # Producto B
    [200, 210, 205, 215]   # Producto C
])

# Indexado b√°sico
ventas[1]           # [80, 90, 95, 100] - todas las ventas del producto B
ventas[2, 3]        # 215 - ventas del producto C en el mes 4
ventas[:, 0]        # [100, 80, 200] - ventas de todos en el primer mes

# Slicing (rebanadas)
ventas[:2, 1:3]     # Primeros 2 productos, meses 2 y 3
# [[120, 150],
#  [90,  95]]

# M√°scaras booleanas (filtrado condicional)
ventas_bajas = ventas < 100          # matriz booleana
productos_bajos = ventas[ventas_bajas]  # [80, 90, 95] - solo valores <100

# Ejemplo pr√°ctico: encontrar productos que superaron meta de 150
supera_meta = ventas > 150
print(f"Productos con ventas >150: {supera_meta.sum()} casos")
# Resultado: 3 casos (producto A en mes 3, producto C en meses 2, 4)

# Asignaci√≥n condicional
ventas_ajustadas = ventas.copy()
ventas_ajustadas[ventas_ajustadas < 100] = 100  # piso m√≠nimo de 100
```

### 3.3 Broadcasting (operar con distintas formas)
**Broadcasting** permite operar arrays de diferentes dimensiones sin copiar datos expl√≠citamente.

```python
# Caso 1: Agregar un valor escalar a un array
precios = np.array([100, 200, 150])
precios_con_iva = precios * 1.21  # broadcast del escalar
# [121., 242., 181.5]

# Caso 2: Sumar un vector fila y un vector columna
x = np.array([1, 2, 3])           # forma (3,)
y = np.array([[10], [20], [30]])  # forma (3, 1) - columna

# NumPy autom√°ticamente expande x a [[1,2,3],[1,2,3],[1,2,3]]
# y expande y a [[10,10,10],[20,20,20],[30,30,30]]
resultado = x + y
# [[11, 12, 13],
#  [21, 22, 23],
#  [31, 32, 33]]

# Caso 3: Aplicar descuentos por producto (columna) y por canal (fila)
productos = 3
canales = 4
ventas_base = np.ones((productos, canales)) * 1000  # ventas base de 1000

# Descuentos por producto (10%, 15%, 20%)
descuento_prod = np.array([[0.9], [0.85], [0.8]])  # (3,1)

# Descuentos adicionales por canal (5%, 0%, 10%, 15%)
descuento_canal = np.array([0.95, 1.0, 0.9, 0.85])  # (4,)

# Broadcasting autom√°tico
ventas_finales = ventas_base * descuento_prod * descuento_canal
# Producto 0: [855, 900, 810, 765]
# Producto 1: [807.5, 850, 765, 722.5]
# Producto 2: [760, 800, 720, 680]
```

**Regla de broadcasting:** NumPy compara dimensiones de **derecha a izquierda**:
- Dimensiones iguales o una de ellas es 1 ‚Üí compatible
- Si una dimensi√≥n no existe, se trata como 1

### 3.4 Agregaciones y estad√≠sticas
```python
# Simular datos de producci√≥n diaria de una planta (30 d√≠as)
produccion = np.random.randint(800, 1200, size=30)

# Estad√≠sticas b√°sicas
print(f"Media: {produccion.mean():.1f} unidades/d√≠a")
print(f"Desviaci√≥n est√°ndar: {produccion.std():.1f}")
print(f"M√≠nimo: {produccion.min()}, M√°ximo: {produccion.max()}")
print(f"Mediana: {np.median(produccion):.1f}")
print(f"Percentil 95: {np.percentile(produccion, 95):.1f}")

# Agregaciones por eje en matrices
ventas_trimestre = np.array([
    [100, 120, 150],  # Producto A (3 meses)
    [80,  90,  95],   # Producto B
    [200, 210, 205]   # Producto C
])

# axis=0: agrupa por columnas (resultado: 1 valor por mes)
por_mes = ventas_trimestre.sum(axis=0)  # [380, 420, 450]

# axis=1: agrupa por filas (resultado: 1 valor por producto)
por_producto = ventas_trimestre.sum(axis=1)  # [370, 265, 615]

# Sin axis: aplana todo
total = ventas_trimestre.sum()  # 1250

# Ejemplo pr√°ctico: identificar productos con ventas inconsistentes
cv = ventas_trimestre.std(axis=1) / ventas_trimestre.mean(axis=1)  # coef. de variaci√≥n
print(f"Coeficiente de variaci√≥n por producto: {cv}")
# Valores altos indican ventas err√°ticas
```

### 3.5 Ufuncs y vectorizaci√≥n
**Ufuncs** (universal functions) son funciones que operan elemento a elemento sobre arrays, implementadas en C.

```python
# Ejemplo 1: Calcular descuento progresivo
cantidades = np.array([5, 15, 25, 50, 100])
# Descuento: 0% hasta 10 unidades, luego 10% por cada 10 adicionales
descuento = np.minimum(np.maximum(cantidades - 10, 0) / 10 * 0.1, 0.5)
# [0, 0.05, 0.15, 0.4, 0.5]

# Ejemplo 2: Funci√≥n log√≠stica (sigmoid) para probabilidades
x = np.linspace(-6, 6, 13)
probabilidad = 1 / (1 + np.exp(-x))  # ufunc exp
# Valores cercanos a 0 para x<0, cercanos a 1 para x>0

# Ejemplo 3: Normalizar precios a escala 0-1
precios = np.array([100, 250, 180, 320, 150])
precios_norm = (precios - precios.min()) / (precios.max() - precios.min())
# [0.0, 0.68, 0.36, 1.0, 0.23]

# Ejemplo 4: Calcular distancias euclidianas (geometr√≠a)
puntos_a = np.array([[0, 0], [1, 1], [2, 2]])
puntos_b = np.array([[3, 3], [4, 4], [5, 5]])
distancias = np.sqrt(((puntos_a - puntos_b)**2).sum(axis=1))
# [4.24, 4.24, 4.24]

# Comparaci√≥n de rendimiento
import time
# Python puro
x_list = list(range(1_000_000))
t0 = time.time()
y_list = [xi**2 + 2*xi + 1 for xi in x_list]
t_python = time.time() - t0

# NumPy vectorizado
x_array = np.arange(1_000_000)
t0 = time.time()
y_array = x_array**2 + 2*x_array + 1
t_numpy = time.time() - t0

print(f"Python: {t_python:.4f}s, NumPy: {t_numpy:.4f}s")
print(f"Speedup: {t_python/t_numpy:.1f}x")
# T√≠picamente 50-100x m√°s r√°pido
```

### 3.6 √Ålgebra lineal y n√∫meros aleatorios
```python
# √Ålgebra lineal: resolver sistema de ecuaciones
# 3x + y = 9
# 2y = 8
A = np.array([[3, 1], [0, 2]], dtype=float)
b = np.array([9, 8], dtype=float)
solucion = np.linalg.solve(A, b)  # x=2, y=4

# Ejemplo pr√°ctico: regresi√≥n lineal simple (m√©todo de m√≠nimos cuadrados)
# Modelo: precio = a + b*√°rea
areas = np.array([50, 70, 80, 100, 120])  # m¬≤
precios = np.array([150, 200, 220, 280, 320])  # miles de $

# Construir matriz X (con columna de 1s para el t√©rmino independiente)
X = np.column_stack([np.ones(len(areas)), areas])
# Soluci√≥n: Œ≤ = (X^T X)^-1 X^T y
beta = np.linalg.inv(X.T @ X) @ X.T @ precios
# beta[0] = intercepto, beta[1] = pendiente
print(f"Precio = {beta[0]:.1f} + {beta[1]:.2f} * √°rea")

# N√∫meros aleatorios (generador moderno)
rng = np.random.default_rng(seed=42)  # reproducibilidad

# Distribuciones comunes
normal = rng.normal(loc=100, scale=15, size=1000)      # Œº=100, œÉ=15
uniforme = rng.uniform(low=0, high=1, size=100)        # entre 0 y 1
enteros = rng.integers(low=1, high=7, size=1000)       # dados (1-6)

# Ejemplo: simular cartera de inversiones (rendimientos diarios)
dias = 252  # a√±o burs√°til
activos = 5
rendimientos = rng.normal(loc=0.0005, scale=0.02, size=(dias, activos))
valor_inicial = 10000
valores = valor_inicial * (1 + rendimientos).cumprod(axis=0)
print(f"Valor final promedio: ${valores[-1].mean():.2f}")
```

**Buenas pr√°cticas con NumPy:**
- Prioriza **operaciones vectorizadas** sobre bucles Python.
- Cuida los **dtypes** (int32/64, float32/64) y **copias** vs **vistas**.
- Usa **broadcasting** para evitar `for` anidados.

---

## 4) Pandas

### 4.0 ¬øQu√© es Pandas y qu√© problema resuelve?

**Pandas** (*Python Data Analysis Library*) es la herramienta est√°ndar para manipulaci√≥n y an√°lisis de datos tabulares y series temporales en Python.

#### Filosof√≠a y prop√≥sito
- **Datos etiquetados:** A diferencia de NumPy (solo √≠ndices num√©ricos), Pandas permite **√≠ndices personalizados** (fechas, IDs, categor√≠as) para acceso intuitivo.
- **Estructuras de alto nivel:** **Series** (1D) y **DataFrame** (2D) abstraen el trabajo con tablas, similar a SQL o Excel pero con m√°s poder.
- **Operaciones relacionales:** Joins, groupby, pivot, reshape ‚Äî todo lo que har√≠as en SQL, pero en Python.
- **Integraci√≥n completa:** Lee/escribe CSV, Excel, SQL, Parquet, JSON; se conecta con NumPy, Matplotlib y scikit-learn.

#### ¬øQu√© problema resuelve?
NumPy es excelente para arrays num√©ricos homog√©neos, pero trabajar con **datos del mundo real** requiere:
- **Columnas de tipos mixtos** (texto, n√∫meros, fechas, categor√≠as).
- **Valores faltantes** (NaN, None) manejados correctamente.
- **Etiquetas descriptivas** (nombres de columnas, √≠ndices con sentido).
- **Operaciones de alto nivel** (filtrado, agrupaci√≥n, joins, reshape).

**Ejemplo del problema:**
```python
# NumPy: dif√≠cil manejar datos heterog√©neos
import numpy as np
# Array con nombres y edades ‚Äî ¬°todo se convierte a strings!
datos = np.array([["Ana", 25], ["Juan", 30], ["Mar√≠a", 28]])
# Calcular edad promedio requiere conversi√≥n manual
edades = datos[:, 1].astype(int)
promedio = edades.mean()  # tedioso y propenso a errores

# Pandas: natural y expresivo
import pandas as pd
df = pd.DataFrame({
    "nombre": ["Ana", "Juan", "Mar√≠a"],
    "edad": [25, 30, 28]
})
promedio = df["edad"].mean()  # 27.67 ‚Äî directo y sin conversiones
mayores = df[df["edad"] > 26]  # filtrado intuitivo
```

**Pandas resuelve:**
1. **Heterogeneidad de tipos:** Cada columna tiene su propio dtype.
2. **Datos faltantes:** Manejo robusto de NaN/None con m√©todos como `dropna()`, `fillna()`.
3. **Operaciones de negocio:** Joins, groupby, pivot, ventanas m√≥viles sin SQL.
4. **Legibilidad:** C√≥digo declarativo con method chaining (`.assign().query().groupby()`).
5. **I/O variado:** Integraci√≥n directa con CSV, Excel, bases SQL, APIs REST.

---

### 4.1 Estructuras y creaci√≥n
```python
import pandas as pd

s = pd.Series([10, 20, 30], index=["a", "b", "c"], name="ventas")
df = pd.DataFrame({
    "producto": ["A", "B", "A", "C"],
    "precio": [10.0, 12.5, 9.5, 7.0],
    "cantidad": [2, 1, 3, 5]
})
```

### 4.2 Carga/descarga de datos (I/O)
```python
df = pd.read_csv("datos.csv")          # sep=";", decimal="," si es necesario
df.to_parquet("datos.parquet")         # columnar, comprimido
df_sql = pd.read_sql("SELECT * FROM tabla", con=engine)
```

### 4.3 Selecci√≥n y filtrado

**Problema com√∫n:** acceder a filas/columnas de forma intuitiva y filtrar por condiciones.

```python
# Dataset de ejemplo: ventas de una tienda
df = pd.DataFrame({
    "producto": ["Laptop", "Mouse", "Teclado", "Monitor", "Laptop"],
    "categoria": ["Electr√≥nica", "Accesorio", "Accesorio", "Electr√≥nica", "Electr√≥nica"],
    "precio": [800, 25, 45, 300, 750],
    "cantidad": [2, 10, 5, 3, 1],
    "vendedor": ["Ana", "Juan", "Ana", "Mar√≠a", "Juan"]
})

# 1. Selecci√≥n de columnas (por nombre)
df["producto"]                    # Serie con una columna
df[["producto", "precio"]]        # DataFrame con dos columnas

# 2. loc: selecci√≥n por etiquetas (nombres de filas/columnas)
# √ötil cuando: trabajas con √≠ndices personalizados (fechas, IDs)
df.loc[0:2, "producto"]           # filas 0 a 2 (inclusivo), columna producto
df.loc[df["precio"] > 100, ["producto", "precio"]]  # filtro + columnas
# Resultado: Laptop (800), Monitor (300), Laptop (750)

# 3. iloc: selecci√≥n por posici√≥n num√©rica (como NumPy)
# √ötil cuando: necesitas las primeras/√∫ltimas N filas sin importar el √≠ndice
df.iloc[0:3]                      # primeras 3 filas (0,1,2 - exclusivo el final)
df.iloc[:, 1:3]                   # todas las filas, columnas 1 y 2 (categoria, precio)

# 4. query: filtrado con sintaxis SQL-like (m√°s legible)
# √ötil cuando: condiciones complejas con m√∫ltiples columnas
df.query("precio > 50 and cantidad >= 2")
# Resultado: Laptop (800, 2), Monitor (300, 3)

df.query("categoria == 'Accesorio' or vendedor == 'Ana'")
# Mouse, Teclado, Laptop (primera), Monitor

# 5. isin: filtrar por pertenencia a lista
# √ötil cuando: tienes un conjunto de valores permitidos/prohibidos
productos_interes = ["Laptop", "Monitor"]
df[df["producto"].isin(productos_interes)]

categorias_excluir = ["Accesorio"]
df[~df["categoria"].isin(categorias_excluir)]  # ~ niega la condici√≥n

# 6. Combinaci√≥n de condiciones
# & (AND), | (OR), ~ (NOT) ‚Äî usar par√©ntesis obligatorios
df[(df["precio"] > 50) & (df["cantidad"] >= 2)]
df[(df["vendedor"] == "Ana") | (df["vendedor"] == "Juan")]

# Ejemplo pr√°ctico: an√°lisis de ventas altas
ventas_importantes = df.query("precio * cantidad > 500")
# Laptop (1600), Monitor (900), Laptop (750)
print(f"Ventas importantes: {len(ventas_importantes)} transacciones")
print(f"Valor total: ${(ventas_importantes['precio'] * ventas_importantes['cantidad']).sum()}")
```

**Diferencias clave:**
- `df["col"]` ‚Üí selecci√≥n r√°pida de columna
- `loc` ‚Üí por etiquetas (√≠ndice y nombres de columnas)
- `iloc` ‚Üí por posici√≥n num√©rica (0, 1, 2...)
- `query` ‚Üí expresiones de texto, √∫til para condiciones complejas legibles

### 4.4 Transformaciones comunes

**Problema com√∫n:** crear nuevas columnas, renombrar, eliminar duplicados, manejar valores faltantes.

```python
# Dataset de ejemplo
df = pd.DataFrame({
    "producto": ["Laptop", "Mouse", "Teclado", "Mouse", "Laptop"],
    "precio": [800, 25, 45, 25, None],  # valor faltante
    "cantidad": [2, 10, 5, 10, 1]
})

# 1. assign: crear/modificar columnas (method chaining)
# √ötil cuando: encadenas m√∫ltiples transformaciones
df = df.assign(
    importe=lambda d: d["precio"] * d["cantidad"],     # nueva columna
    precio_con_iva=lambda d: d["precio"] * 1.21,       # otra columna
    categoria=lambda d: d["producto"].map({             # mapeo condicional
        "Laptop": "Electr√≥nica",
        "Mouse": "Accesorio",
        "Teclado": "Accesorio"
    })
)

# Alternativa sin assign (menos elegante pero com√∫n)
df["importe"] = df["precio"] * df["cantidad"]

# 2. rename: cambiar nombres de columnas
# √ötil cuando: importas datos con nombres en ingl√©s o poco claros
df = df.rename(columns={
    "precio": "precio_unitario",
    "cantidad": "unidades_vendidas"
})

# 3. drop_duplicates: eliminar filas repetidas
# √ötil cuando: datos de logs, transacciones con errores de carga
df_sin_duplicados = df.drop_duplicates()
# Eliminar duplicados solo en ciertas columnas
df_productos_unicos = df.drop_duplicates(subset=["producto"])

# 4. Valores faltantes (NaN/None)
# dropna: eliminar filas con valores faltantes
df_completo = df.dropna()                     # elimina todas las filas con alg√∫n NaN
df_completo = df.dropna(subset=["precio"])    # solo si falta precio

# fillna: rellenar valores faltantes
df["precio"] = df["precio"].fillna(df["precio"].mean())  # con promedio
df["precio"] = df["precio"].fillna(0)                     # con cero
df["precio"] = df["precio"].fillna(method="ffill")        # forward fill (√∫ltimo v√°lido)

# 5. Operaciones con strings (vectorizadas)
# √ötil cuando: limpiar textos, estandarizar
df["producto"] = df["producto"].str.upper()               # LAPTOP, MOUSE
df["producto"] = df["producto"].str.strip()               # quitar espacios
df["producto_limpio"] = df["producto"].str.replace("√°", "a")  # normalizar

# Extraer informaci√≥n
df["largo_nombre"] = df["producto"].str.len()
df["empieza_con_L"] = df["producto"].str.startswith("L")

# 6. Ejemplo pr√°ctico: preparar datos para an√°lisis
df_limpio = (
    df.drop_duplicates()                                   # sin duplicados
      .dropna(subset=["precio", "cantidad"])              # sin faltantes cr√≠ticos
      .assign(
          producto=lambda d: d["producto"].str.upper().str.strip(),
          importe=lambda d: d["precio"] * d["cantidad"],
          es_alto_valor=lambda d: d["importe"] > 500
      )
      .query("cantidad > 0")                              # solo cantidades v√°lidas
)

print(f"Filas originales: {len(df)}, despu√©s de limpieza: {len(df_limpio)}")
```

**Cu√°ndo usar cada operador:**
- `assign` ‚Üí crear columnas en cadenas (pipeline declarativo)
- `rename` ‚Üí estandarizar nombres de columnas al inicio
- `drop_duplicates` ‚Üí despu√©s de cargar datos, antes de an√°lisis
- `dropna/fillna` ‚Üí estrategia seg√∫n negocio (eliminar vs imputar)
- `str.*` ‚Üí limpieza de textos (may√∫sculas, espacios, reemplazos)

### 4.5 Tipos, fechas y categor√≠as
```python
df["fecha"] = pd.to_datetime(df["fecha"], dayfirst=True, errors="coerce")
df["producto"] = df["producto"].astype("category")
df["anio"] = df["fecha"].dt.year
```

### 4.6 GroupBy (agregaciones y transform)

**Problema com√∫n:** calcular totales, promedios, conteos por categor√≠as (SQL GROUP BY).

```python
# Dataset: ventas de una cadena de tiendas
df = pd.DataFrame({
    "tienda": ["Norte", "Sur", "Norte", "Centro", "Sur", "Norte", "Centro"],
    "producto": ["Laptop", "Mouse", "Teclado", "Laptop", "Mouse", "Laptop", "Teclado"],
    "vendedor": ["Ana", "Juan", "Ana", "Mar√≠a", "Juan", "Pedro", "Mar√≠a"],
    "precio": [800, 25, 45, 850, 20, 780, 50],
    "cantidad": [2, 10, 5, 1, 15, 3, 4],
    "fecha": pd.to_datetime([
        "2025-01-15", "2025-01-16", "2025-01-15",
        "2025-01-17", "2025-01-16", "2025-01-18", "2025-01-17"
    ])
})
df["importe"] = df["precio"] * df["cantidad"]

# 1. Agregaci√≥n simple (una m√©trica, un grupo)
# ¬øCu√°nto vendi√≥ cada tienda?
ventas_por_tienda = df.groupby("tienda")["importe"].sum()
# Norte: 3990, Sur: 550, Centro: 4050

# 2. M√∫ltiples agregaciones con agg()
# √ötil cuando: necesitas varias m√©tricas (suma, promedio, conteo) a la vez
resumen = df.groupby("tienda").agg(
    ventas_totales=("importe", "sum"),
    ticket_promedio=("importe", "mean"),
    transacciones=("importe", "count"),
    unidades=("cantidad", "sum")
).reset_index()

# Resultado:
#    tienda  ventas_totales  ticket_promedio  transacciones  unidades
# 0  Centro          4050.0           2025.0              2         5
# 1   Norte          3990.0           1330.0              3        10
# 2     Sur           550.0            275.0              2        25

# 3. Agrupar por m√∫ltiples columnas
# ¬øCu√°nto vendi√≥ cada vendedor en cada tienda?
por_tienda_vendedor = df.groupby(["tienda", "vendedor"])["importe"].sum()
# Norte-Ana: 1825, Norte-Pedro: 2340, Sur-Juan: 550, etc.

# Con reset_index() para convertir √≠ndice multinivel en columnas
tabla = df.groupby(["tienda", "vendedor"]).agg(
    ventas=("importe", "sum")
).reset_index()

# 4. transform: agregar columnas calculadas por grupo (mantiene tama√±o original)
# √ötil cuando: quieres calcular % de participaci√≥n, desviaciones del promedio del grupo
df["total_tienda"] = df.groupby("tienda")["importe"].transform("sum")
df["participacion"] = (df["importe"] / df["total_tienda"]) * 100

# Resultado: cada fila tiene su importe y el % que representa del total de su tienda
# Ej: primera venta (Norte, 1600) representa 40.1% del total de Norte (3990)

# 5. Comparar con promedio del grupo
df["importe_prom_tienda"] = df.groupby("tienda")["importe"].transform("mean")
df["vs_promedio"] = df["importe"] - df["importe_prom_tienda"]
# Valores positivos: venta superior al promedio de la tienda

# 6. filter: quedarse solo con grupos que cumplen condici√≥n
# √ötil cuando: eliminar tiendas con pocas ventas, clientes inactivos
# Solo tiendas con ventas totales > 1000
tiendas_importantes = df.groupby("tienda").filter(lambda g: g["importe"].sum() > 1000)

# 7. Ejemplo pr√°ctico: ranking de productos por tienda
df["rank_en_tienda"] = df.groupby("tienda")["importe"].rank(ascending=False, method="dense")
top_productos = df[df["rank_en_tienda"] == 1]  # producto m√°s vendido por tienda

# 8. Agregaciones temporales
ventas_diarias = df.groupby(df["fecha"].dt.date)["importe"].sum()
# 2025-01-15: 2025, 2025-01-16: 800, etc.

# 9. M√∫ltiples funciones por columna
detalle = df.groupby("producto").agg({
    "importe": ["sum", "mean", "count"],
    "cantidad": ["sum", "mean"]
})
# Columnas multinivel: (importe, sum), (importe, mean), etc.
detalle.columns = ["_".join(col) for col in detalle.columns]  # aplanar nombres
```

**Diferencias clave:**
- **agg()** ‚Üí reduce filas (una fila por grupo) ‚Üí resultados agregados
- **transform()** ‚Üí mantiene el tama√±o original ‚Üí agregar columnas calculadas por grupo
- **filter()** ‚Üí mantiene/elimina grupos completos seg√∫n condici√≥n
- **apply()** ‚Üí flexibilidad total, pero m√°s lento (evitar si hay alternativa vectorizada)

### 4.7 Joins/Merge y concatenaci√≥n

**Problema com√∫n:** combinar datos de m√∫ltiples tablas (como SQL JOIN), apilar datasets.

```python
# Tablas de ejemplo
# Maestro de productos
productos = pd.DataFrame({
    "producto_id": [1, 2, 3, 4],
    "nombre": ["Laptop", "Mouse", "Teclado", "Monitor"],
    "categoria": ["Electr√≥nica", "Accesorio", "Accesorio", "Electr√≥nica"],
    "costo": [600, 10, 20, 200]
})

# Transacciones de ventas
ventas = pd.DataFrame({
    "venta_id": [101, 102, 103, 104, 105],
    "producto_id": [1, 2, 1, 5, 3],  # nota: 5 no existe en productos
    "cantidad": [2, 10, 1, 3, 5],
    "precio_venta": [800, 25, 850, 120, 45]
})

# 1. INNER JOIN (solo registros con match en ambas tablas)
# √ötil cuando: solo quieres transacciones de productos v√°lidos
inner = ventas.merge(productos, on="producto_id", how="inner")
# Resultado: 4 filas (venta_id 104 excluida porque producto_id=5 no existe)

print(f"Ventas totales: {len(ventas)}, con info completa: {len(inner)}")

# 2. LEFT JOIN (todas las ventas, info de productos si existe)
# √ötil cuando: quieres mantener todas las transacciones y marcar hu√©rfanas
left = ventas.merge(productos, on="producto_id", how="left")
# Resultado: 5 filas, venta_id 104 tiene NaN en nombre/categoria/costo

# Identificar ventas sin producto v√°lido
huerfanas = left[left["nombre"].isna()]
print(f"Ventas sin producto registrado: {len(huerfanas)}")

# 3. RIGHT JOIN (todos los productos, ventas si existen)
# √ötil cuando: auditar productos sin movimiento
right = ventas.merge(productos, on="producto_id", how="right")
# Resultado: Monitor (producto_id=4) aparece con NaN en venta_id, cantidad, precio

sin_ventas = right[right["venta_id"].isna()]
print(f"Productos sin ventas: {sin_ventas['nombre'].tolist()}")

# 4. OUTER JOIN (todos los registros de ambas tablas)
# √ötil cuando: an√°lisis completo, quieres ver todo
outer = ventas.merge(productos, on="producto_id", how="outer")
# Resultado: 6 filas (5 ventas + 1 producto sin venta)

# 5. Join con columnas de nombres diferentes
# Ejemplo: columna se llama "prod_id" en una tabla y "id" en otra
# ventas.merge(productos, left_on="prod_id", right_on="id", how="left")

# 6. Join por m√∫ltiples columnas
# Ejemplo: tienda + producto
tiendas_productos = pd.DataFrame({
    "tienda": ["Norte", "Sur", "Norte"],
    "producto_id": [1, 2, 1],
    "stock": [5, 20, 3]
})
ventas_con_stock = ventas.merge(
    tiendas_productos,
    on=["producto_id"],  # si tambi√©n tuvieras "tienda" en ventas
    how="left"
)

# 7. Concatenaci√≥n vertical (apilar filas)
# √ötil cuando: unir datos de m√∫ltiples per√≠odos, sucursales, fuentes
ventas_enero = pd.DataFrame({
    "producto": ["Laptop", "Mouse"],
    "importe": [1600, 250]
})
ventas_febrero = pd.DataFrame({
    "producto": ["Teclado", "Monitor"],
    "importe": [225, 900]
})
ventas_bimestre = pd.concat([ventas_enero, ventas_febrero], axis=0, ignore_index=True)
# Resultado: 4 filas con √≠ndice 0,1,2,3

# 8. Concatenaci√≥n horizontal (agregar columnas)
# √ötil cuando: datos de mismas filas en archivos separados
info_basica = pd.DataFrame({"id": [1,2], "nombre": ["Ana", "Juan"]})
info_contacto = pd.DataFrame({"email": ["ana@x.com", "juan@y.com"]})
completo = pd.concat([info_basica, info_contacto], axis=1)

# 9. Ejemplo pr√°ctico: an√°lisis con margen
# Calcular margen por venta (precio_venta - costo)
analisis = (
    ventas.merge(productos, on="producto_id", how="left")
           .assign(
               importe=lambda d: d["precio_venta"] * d["cantidad"],
               costo_total=lambda d: d["costo"] * d["cantidad"],
               margen=lambda d: (d["precio_venta"] - d["costo"]) * d["cantidad"],
               margen_pct=lambda d: ((d["precio_venta"] - d["costo"]) / d["precio_venta"]) * 100
           )
)

# Resumen por categor√≠a
resumen_categoria = analisis.groupby("categoria").agg(
    ventas_totales=("importe", "sum"),
    margen_total=("margen", "sum"),
    transacciones=("venta_id", "count")
).reset_index()

print(resumen_categoria)
# Electr√≥nica: 3550 ventas, 1350 margen
# Accesorio: 400 ventas, 200 margen
```

**Tipos de join - cu√°ndo usar:**
- `inner` ‚Üí solo registros v√°lidos en ambas tablas (intersecci√≥n)
- `left` ‚Üí mantener todo de la izquierda, agregar info de derecha si existe
- `right` ‚Üí mantener todo de la derecha, agregar info de izquierda si existe
- `outer` ‚Üí uni√≥n completa (todos los registros de ambas)

**Buenas pr√°cticas:**
- Siempre **validar** el n√∫mero de filas despu√©s de merge (puede explotar por duplicados)
- Usar `indicator=True` para ver origen de cada fila: `merge(..., indicator=True)`
- Manejar NaN resultantes de joins con `fillna()` o filtros

### 4.8 Pivot, melt, ventanas y resample

**Problema com√∫n:** transformar entre formato largo/ancho, calcular promedios m√≥viles, agregar series temporales.

```python
import pandas as pd
import numpy as np

# Dataset: ventas mensuales por producto
ventas = pd.DataFrame({
    "fecha": pd.to_datetime(["2025-01", "2025-02", "2025-03", "2025-01", "2025-02", "2025-03"]),
    "producto": ["Laptop", "Laptop", "Laptop", "Mouse", "Mouse", "Mouse"],
    "ventas": [10, 12, 15, 50, 55, 60],
    "region": ["Norte", "Norte", "Norte", "Sur", "Sur", "Sur"]
})

# 1. PIVOT: formato largo ‚Üí ancho (Excel pivot table)
# √ötil cuando: presentar datos en tabla legible, cada producto una columna
pivot = ventas.pivot_table(
    index="fecha",           # filas
    columns="producto",      # columnas
    values="ventas",         # valores
    aggfunc="sum",           # funci√≥n de agregaci√≥n
    fill_value=0             # rellenar vac√≠os con 0
)

# Resultado:
# producto     Laptop  Mouse
# fecha
# 2025-01-01       10     50
# 2025-02-01       12     55
# 2025-03-01       15     60

# Pivot con m√∫ltiples valores
pivot_multi = ventas.pivot_table(
    index="fecha",
    columns="producto",
    values="ventas",
    aggfunc=["sum", "mean"]  # m√∫ltiples agregaciones
)

# 2. MELT: formato ancho ‚Üí largo (unpivot)
# √ötil cuando: necesitas formato "tidy" para gr√°ficos o an√°lisis
ancho = pd.DataFrame({
    "producto": ["Laptop", "Mouse"],
    "enero": [10, 50],
    "febrero": [12, 55],
    "marzo": [15, 60]
})

largo = ancho.melt(
    id_vars="producto",       # columnas que no se derriten
    var_name="mes",           # nombre para columna de meses
    value_name="ventas"       # nombre para los valores
)

# Resultado:
#   producto      mes  ventas
# 0   Laptop    enero      10
# 1    Mouse    enero      50
# 2   Laptop  febrero      12
# ...

# 3. ROLLING: ventanas m√≥viles (promedios, sumas m√≥viles)
# √ötil cuando: suavizar series temporales, detectar tendencias
ventas_diarias = pd.DataFrame({
    "fecha": pd.date_range("2025-01-01", periods=30, freq="D"),
    "ventas": np.random.randint(80, 120, 30)
})

# Promedio m√≥vil de 7 d√≠as
ventas_diarias["promedio_7d"] = ventas_diarias["ventas"].rolling(
    window=7,
    min_periods=1  # calcular incluso con menos de 7 datos al inicio
).mean()

# Suma acumulada (rolling infinito)
ventas_diarias["acumulado"] = ventas_diarias["ventas"].cumsum()

# M√°ximo en ventana de 5 d√≠as
ventas_diarias["max_5d"] = ventas_diarias["ventas"].rolling(5).max()

# Ejemplo pr√°ctico: detectar picos de ventas
ventas_diarias["desviacion"] = (
    ventas_diarias["ventas"] - ventas_diarias["promedio_7d"]
)
picos = ventas_diarias[ventas_diarias["desviacion"] > 15]  # >15 del promedio
print(f"D√≠as con picos de ventas: {len(picos)}")

# 4. RESAMPLE: agregar series temporales (equivalente a GROUP BY temporal)
# √ötil cuando: datos diarios ‚Üí mensuales, por hora ‚Üí diarios
# Requiere que el √≠ndice sea datetime
ts = ventas_diarias.set_index("fecha")

# Agregar a nivel mensual
mensual = ts.resample("M").agg({
    "ventas": "sum",                     # suma mensual
    "promedio_7d": "mean",               # promedio de los promedios
    "acumulado": "last"                  # valor al final del mes
})

# Frecuencias comunes: "D" (d√≠a), "W" (semana), "M" (mes), "Q" (trimestre), "Y" (a√±o)
semanal = ts.resample("W").sum()        # suma semanal
trimestral = ts.resample("Q").mean()    # promedio trimestral

# Resample con funciones personalizadas
mensual_detalle = ts.resample("M").agg({
    "ventas": ["sum", "mean", "min", "max", "count"]
})

# 5. EXPANDING: ventanas expandibles (todo hasta el momento actual)
# √ötil cuando: calcular acumulados progresivos, promedios hist√≥ricos
ventas_diarias["promedio_historico"] = ventas_diarias["ventas"].expanding().mean()
# D√≠a 1: promedio de 1 valor, D√≠a 2: promedio de 2 valores, etc.

# 6. Ejemplo integrador: an√°lisis temporal completo
datos_completos = pd.DataFrame({
    "fecha": pd.date_range("2024-01-01", "2025-03-31", freq="D"),
    "ventas": np.random.randint(50, 150, 455)
})

analisis_temporal = (
    datos_completos
    .set_index("fecha")
    .assign(
        promedio_30d=lambda d: d["ventas"].rolling(30, min_periods=1).mean(),
        tendencia=lambda d: d["ventas"].rolling(7).mean(),
        volatilidad=lambda d: d["ventas"].rolling(30).std(),
        acumulado_anual=lambda d: d.groupby(d.index.year)["ventas"].cumsum()
    )
)

# Resumen mensual con m√©tricas clave
resumen_mensual = analisis_temporal.resample("M").agg({
    "ventas": ["sum", "mean", "std"],
    "promedio_30d": "last",
    "acumulado_anual": "last"
})

print(resumen_mensual.head())

# 7. Pivot con m√∫ltiples √≠ndices
# Ejemplo: ventas por producto, regi√≥n y mes
ventas_complejas = pd.DataFrame({
    "mes": ["Enero", "Enero", "Febrero", "Febrero"] * 2,
    "producto": ["Laptop", "Mouse"] * 4,
    "region": ["Norte", "Norte", "Norte", "Norte", "Sur", "Sur", "Sur", "Sur"],
    "ventas": [10, 50, 12, 55, 8, 45, 10, 50]
})

pivot_complejo = ventas_complejas.pivot_table(
    index=["mes", "producto"],   # √≠ndice multinivel
    columns="region",
    values="ventas",
    aggfunc="sum",
    fill_value=0
)

# Resultado:
# region           Norte  Sur
# mes     producto
# Enero   Laptop      10    8
#         Mouse       50   45
# Febrero Laptop      12   10
#         Mouse       55   50
```

**Cu√°ndo usar cada operaci√≥n:**
- **pivot_table** ‚Üí largo a ancho, para tablas resumen estilo Excel
- **melt** ‚Üí ancho a largo, para preparar datos "tidy" (an√°lisis/gr√°ficos)
- **rolling** ‚Üí promedios m√≥viles, suavizar ruido, detectar tendencias
- **resample** ‚Üí cambiar frecuencia temporal (d√≠a‚Üímes, hora‚Üíd√≠a)
- **expanding** ‚Üí estad√≠sticas acumuladas desde el inicio

**Diferencia rolling vs expanding:**
- `rolling(7)` ‚Üí siempre 7 elementos (ventana fija)
- `expanding()` ‚Üí desde inicio hasta actual (ventana creciente)

### 4.9 Rendimiento y estilo
- Prefiere **vectorizaci√≥n** y **method chaining** (e.g. `.assign().pipe()`).
- Evita `apply` fila a fila salvo necesidad; mejor **ufuncs** o **map** de Series.
- Usa formatos eficientes: **Parquet**/Feather para grandes vol√∫menes.
- Documenta supuestos y unidades, y valida con **asserts** o `pd.testing`.

---

## 5) ¬øC√≥mo se complementan NumPy y Pandas?
- **Pandas** usa **NumPy** internamente para almacenar columnas como arrays; muchas operaciones son **ufuncs** sobre esos arrays.
- Cuando necesites rendimiento num√©rico o funciones matem√°ticas avanzadas, extrae la columna `pd.Series.to_numpy()` y usa NumPy.
- Para √°lgebra lineal, simulaciones o transformaciones vectorizadas complejas, **NumPy** es la base; para datos etiquetados, joins y agregaciones, **Pandas** es la herramienta.

---

## 6) Ejemplo integrador
**Contexto:** an√°lisis de ventas de una PyME con dos tablas: `ventas.csv` (transacciones) y `productos.csv` (maestro). Queremos KPIs por producto/categor√≠a, detectar *outliers* en importes y construir un tablero resumen.

### 6.1 Datos de ejemplo (en memoria)
```python
import numpy as np
import pandas as pd

ventas = pd.DataFrame({
    "id": range(1, 13),
    "fecha": pd.to_datetime([
        "2025-01-02","2025-01-03","2025-01-05","2025-01-10",
        "2025-01-11","2025-01-12","2025-02-01","2025-02-02",
        "2025-02-08","2025-02-10","2025-02-11","2025-02-15"
    ]),
    "producto": ["A","A","B","C","A","B","C","C","A","B","C","A"],
    "precio": [10,10,12,8,11,12.5,7,7.5,10.5,12,7.2,9.8],
    "cantidad": [2,1,1,5,3,2,4,3,2,1,8,10]
})

productos = pd.DataFrame({
    "producto": ["A","B","C"],
    "categoria": ["HOGAR","HOGAR","OFICINA"]
})

ventas["importe"] = ventas["precio"] * ventas["cantidad"]
```

### 6.2 Join + KPIs por producto/categor√≠a + share
```python
df = ventas.merge(productos, on="producto", how="left")

kpis_prod = (
    df.groupby("producto")
      .agg(ventas=("id","count"),
           cant_total=("cantidad","sum"),
           ticket_prom=("importe","mean"),
           facturacion=("importe","sum"))
      .sort_values("facturacion", ascending=False)
)

kpis_cat = (
    df.groupby("categoria")
      .agg(facturacion=("importe","sum"),
           cant_total=("cantidad","sum"))
      .assign(ticket_prom=lambda d: d["facturacion"]/d["cant_total"])
)

df["share_prod"] = df["importe"] / df.groupby("producto")["importe"].transform("sum")
df["mes"] = df["fecha"].dt.to_period("M").astype(str)
kpi_mes_prod = df.pivot_table(index="mes", columns="producto", values="importe", aggfunc="sum", fill_value=0)
```

### 6.3 Outliers con NumPy (z-score)
```python
imp = df["importe"].to_numpy()
mu, sigma = imp.mean(), imp.std(ddof=0)       # NumPy stats
z = (imp - mu) / sigma
df["zscore_importe"] = z
outliers = df[np.abs(df["zscore_importe"]) > 2.0]
```

### 6.4 Ventanas m√≥viles y serie temporal
```python
df = df.sort_values("fecha")
df["rolling_3"] = df["importe"].rolling(3, min_periods=1).sum()

ts_mensual = df.set_index("fecha").resample("M")["importe"].sum()
```

### 6.5 Chequeos de calidad y reporte final
```python
assert not df["importe"].isna().any()
assert (df["cantidad"] > 0).all()

reporte = {
    "kpis_producto": kpis_prod.to_dict(),
    "kpis_categoria": kpis_cat.to_dict(),
    "ventas_por_mes_y_producto": kpi_mes_prod.to_dict(),
    "outliers": outliers[["id","producto","importe","zscore_importe"]].to_dict(orient="records"),
    "facturacion_mensual": ts_mensual.to_dict()
}
reporte
```

> **Extensiones posibles:** agregar costos para margen, integrar con base SQL, graficar KPIs, calcular cohortes o RFM, y guardar resultados en Parquet/CSV.

---

## 7) Errores comunes y buenas pr√°cticas
- Mezclar `loc/iloc` sin entender la diferencia (etiqueta vs posici√≥n).
- Usar `apply` fila a fila cuando una operaci√≥n vectorizada existe.
- No **parsear fechas** (`pd.to_datetime`) ni **tipificar** (`astype('category')`).
- No documentar supuestos de negocio; usar nombres ambiguos.
- Trabajar con CSV enormes sin **chunks** o **Parquet**.
- No testear (usa `pd.testing.assert_frame_equal`).

**Patrones √∫tiles:**
```python
# Method chaining + pipe para legibilidad
def normalizar_importe(df):
    m, s = df["importe"].mean(), df["importe"].std(ddof=0)
    return df.assign(importe_norm=(df["importe"] - m) / s)

res = (
    df.pipe(lambda d: d.query("cantidad > 0"))
      .assign(mes=lambda d: d["fecha"].dt.to_period("M").astype(str))
      .pipe(normalizar_importe)
)
```

---

## 8) Recursos oficiales y tutoriales
- **NumPy**: Documentaci√≥n https://numpy.org/doc/ ‚Äî Tutorial https://numpy.org/devdocs/user/quickstart.html
- **Pandas**: Documentaci√≥n https://pandas.pydata.org/docs/ ‚Äî Tutorial https://pandas.pydata.org/docs/user_guide/index.html
- **SciPy**: https://docs.scipy.org/doc/
- **scikit-learn**: https://scikit-learn.org/stable/
- **Matplotlib**: https://matplotlib.org/stable/
- **Dask**: https://docs.dask.org/en/stable/
- **PySpark**: https://spark.apache.org/docs/latest/api/python/

---

## 9) Checklist de este tema
- [ ] Crear y manipular **ndarrays** (NumPy).
- [ ] Comprender **broadcasting**, ufuncs y agregaciones.
- [ ] Crear/leer **Series** y **DataFrames** (Pandas).
- [ ] Filtrar/transformar con `loc`, `iloc`, `query`, `assign`, `astype`.
- [ ] Hacer **groupby**, **merge**, **concat**, **pivot**, **rolling**.
- [ ] Integrar NumPy + Pandas (z-score, m√©tricas, ventanas).
- [ ] Exportar resultados (CSV/Parquet) y validar datos.

---

> **Actividad sugerida:** Reproducir el ejemplo con tus propios datos (ventas, producci√≥n, calidad o inventario), agregar una m√©trica de negocio y un gr√°fico mensual. Guardar KPIs en Parquet y escribir un breve informe con decisiones sugeridas.
